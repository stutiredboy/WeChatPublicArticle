---
title: 大模型推理精度与显存估算指南
---

# 大模型推理精度与显存估算指南

> 原文链接：[大模型推理精度与显存估算指南](https://mp.weixin.qq.com/s/c0iUgMbBLE-DgcRQThpHqQ)

&nbsp;## 前言



本地跑大模型最痛苦的是什么？不是模型不够强，而是**OOM (Out Of Memory)**——显存炸了。

如果你去HuggingFace/Modelscope上查看某个模型介绍，你肯定看到过&nbsp;`FP32`、`FP16`、`BF16`、`FP8`、`FP4`这些所谓的张量类型，而张量类型是实现模型推理精度的底层数据格式的载体，例如张量类型为`BF16`的模型我们可以使用`FP8`的精度推理。![](images/1770719768571.png)![](images/1770719768614.png)

本文目标就是帮大家将这些概念捋清楚，顺便帮大家算算手里的显卡大概能跑什么参数的模型。


![](images/1770719768994.png)

## 01. 精度：其实就是模型的“分辨率”



模型里的知识都是用数字存的。你可以把这些精度格式想象成图片的**分辨率**。•&nbsp;**FP32 (单精度)**：**4K 原盘电影。**
&nbsp; 每个参数占&nbsp;**4字节**。虽然精准，但体积巨大，推理时基本用不上，那是训练时候用的“母带”。•&nbsp;**FP16 / BF16 (半精度)**：**1080P 高清。**
&nbsp; 每个参数占&nbsp;**2字节**。这是目前推理的**主流标准**。显存占用直接砍半，效果几乎没区别。### 灵魂发问：BF16 和 FP16 有啥区别？



这俩虽然体积一样，但技能点不同：•&nbsp;**FP16**：刻度精细，但尺子短。数字稍微大一点（超过65504）就会**溢出**，导致程序报错（NaN）。•&nbsp;**BF16 (Brain Float)**：谷歌搞出来的“AI 专用尺”。它把精细度牺牲了一点点，但换来了**极大的量程**（和 FP32 一样）。

**“不报错”比“过度精确”更重要**•&nbsp;**FP8 / FP4 (量化)**：**720P / 360P 压缩版。**•&nbsp;**FP8**：占&nbsp;**1字节**。•&nbsp;**FP4**：占&nbsp;**0.5字节**。
&nbsp; 这就是所谓的“量化”。通过降低精度，把几十 GB 的模型硬塞进家用显卡里。虽然画质（智商）略有下降，但对于 70B 这种巨型模型来说，用一点点智商换能跑起来，绝对血赚。## 02. 算账：你的显存够用吗？



模型推理时的显存占用，可通过**基础公式+经验系数**快速估算：

**显存占用 ≈ 模型参数量（B）× 单参数字节数 × 1.2**

这里的&nbsp;**20% 增量**，核心是预留&nbsp;**KV Cache**&nbsp;的空间。#### 以&nbsp;**Qwen3-8B**（80亿参数）为例

精度单参数字节数基础显存计算实际建议显存部署可行性FP162约 19.2 GB16GB 显存卡**极限运行**，极易 OOM（显存溢出）FP81约 9.6 GB轻松适配消费级显卡(如RTX4080/4090)FP40.5约 4.8 GB需Blackwell架构的卡原生支持FP4(如RTX50系列)#### 再看大模型&nbsp;**Qwen3-32B**（320亿参数）

精度基础显存计算实际建议显存部署方案FP16约 76.8 GB消费级单卡直接劝退，需依赖多卡集群或企业级 GPU（如 H20）FP4约 19.2 GB一张 32G 显存的消费级显卡（如 RTX 5090）足可胜任#### 补充说明

1.&nbsp;**KV Cache 影响因子**：1.2 是通用经验值，若对话轮次多、上下文长度长，系数需上调至 1.3~1.5；2.&nbsp;**量化精度权衡**：FP4/FP8 量化虽大幅降低显存，但会轻微损失模型生成精度，需根据业务场景取舍。## 03. 硬件：老黄的刀法与你的显卡



并不是你把模型压成 FP8，显卡就能跑得飞快。这得看显卡硬件支不支持“原生加速”。

来看看你的显卡属于哪一代：•&nbsp;**Pascal (GTX 10系，如 1080Ti)**• 老当益壮，但跑大模型比较吃力。基本只能跑 FP32/FP16，缺乏专门的 Tensor Core 加速。•&nbsp;**Turing (RTX 20系)**• AI 推理的入门级。支持 FP16 和 INT8。•&nbsp;**Ampere (RTX 30系，如 3080/3090)**&nbsp;——&nbsp;**当前普通人性价比之王**•&nbsp;**支持**：FP16、BF16 (部分支持)、INT8。•&nbsp;**不支持**：硬件级 FP8/FP4。•&nbsp;*注意*：你可以在 3090 上跑 FP8 格式的模型来**省显存**，但由于没有硬件加速，计算速度可能不会变快，甚至不如跑 FP16/INT8 快。•&nbsp;**Ada Lovelace (RTX 40系) &amp; Hopper (H100)**•&nbsp;**支持**：原生 FP8。这就是为啥 4090 跑量化模型那么快。•&nbsp;**Blackwell (GB200/B200)**•&nbsp;**支持**：原生 NVFP4 精度，同时完美兼容 FP8、BF16 等前代高精度格式，通过硬件级量化实现极致显存优化与性能提升## 04. 如果手握8张3090可以怎么玩？

•&nbsp;**8张 3090 (192GB 总显存)**：你可以直接用&nbsp;**FP16 原生精度**&nbsp;跑&nbsp;`Llama-3-70B`&nbsp;(约需168G显存)，无需任何量化，享受“满血版”模型的快乐。或者尝试运行更大的量化版模型如`Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix`。•&nbsp;**避坑指南**：
&nbsp; 在 30 系显卡上，优先选择&nbsp;**GPTQ-Int4**&nbsp;或&nbsp;**AWQ-Int4**&nbsp;这种量化格式，而不是纯粹的 FP8。因为 30 系对 Int8/Int4 的支持比 FP8 成熟得多，速度也更快。

**总结一下：**
显存不够，量化来凑；显卡够强，BF16 甚至 FP16 才是满血信仰。


![](images/1770719769046.png)

*觉得有用的话，欢迎“点赞👍”、“转发”、“收藏”，下次聊聊怎么用 vLLM 把你的多张显卡串起来。*

&nbsp;

